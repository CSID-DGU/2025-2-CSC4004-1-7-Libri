import argparse
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime

from config import (
    DEVICE, N_AGENTS, WINDOW_SIZE, BUFFER_SIZE, BATCH_SIZE, 
    TARGET_UPDATE_FREQ, NUM_EPISODES
)
from data_processor import DataProcessor
from environment import MARLStockEnv
from qmix_model import QMIX_Learner
from replay_buffer import ReplayBuffer

# --- ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê·¸ë˜í”„ í•¨ìˆ˜ ---
def plot_backtest_results(portfolio_values, daily_pnls, test_prices, initial_capital):
    """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
    dates = test_prices.index[:len(portfolio_values)]
    
    # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
    returns = pd.Series(daily_pnls) / initial_capital
    
    # Sharpe Ratio
    sharpe = (returns.mean() / (returns.std() + 1e-9)) * np.sqrt(252)
    
    # Sortino Ratio (í•˜ë°© ë³€ë™ì„±ë§Œ ê³ ë ¤)
    downside_returns = returns[returns < 0]
    downside_std = downside_returns.std() if len(downside_returns) > 0 else 1e-9
    sortino = (returns.mean() / (downside_std + 1e-9)) * np.sqrt(252)
    
    # MDD (Maximum Drawdown)
    cumulative = np.array(portfolio_values)
    running_max = np.maximum.accumulate(cumulative)
    drawdown = (cumulative - running_max) / running_max
    mdd = drawdown.min() * 100
    
    # KOSPI ë²¤ì¹˜ë§ˆí¬ (Buy & Hold)
    kospi_start = test_prices.iloc[0]
    kospi_values = [(initial_capital / kospi_start) * price for price in test_prices.iloc[:len(portfolio_values)]]
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, ax = plt.subplots(figsize=(14, 8))
    
    # ì œëª©ê³¼ ì„±ê³¼ ì§€í‘œ
    title = f'QMIX 4-Agent ë°±í…ŒìŠ¤íŠ¸ ì„±ê³¼\nì´ˆê¸°ìê¸ˆ: {initial_capital:,.0f}ì› | Sharpe: {sharpe:.3f} | Sortino: {sortino:.3f} | MDD: {mdd:.2f}%'
    fig.suptitle(title, fontsize=14, fontweight='bold', y=0.98)
    
    # QMIX Agent í¬íŠ¸í´ë¦¬ì˜¤
    ax.plot(dates, portfolio_values, label='QMIX Agent', color='#2E86AB', linewidth=2.5, zorder=3)
    
    # KOSPI ë²¤ì¹˜ë§ˆí¬
    ax.plot(dates, kospi_values, label='KOSPI (Buy & Hold)', color='#A23B72', linewidth=2, linestyle='--', alpha=0.8, zorder=2)
    
    # ì´ˆê¸° ìë³¸ ê¸°ì¤€ì„ 
    ax.axhline(y=initial_capital, color='gray', linestyle=':', linewidth=1.5, alpha=0.5, label='ì´ˆê¸° ìë³¸')
    
    # ì¶• ì„¤ì •
    ax.set_xlabel('ë‚ ì§œ', fontsize=12, fontweight='bold')
    ax.set_ylabel('í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ (ì›)', fontsize=12, fontweight='bold')
    ax.legend(loc='best', fontsize=11, framealpha=0.9)
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # Yì¶• í¬ë§· (ë°±ë§Œ ì› ë‹¨ìœ„)
    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))
    
    # Xì¶• í¬ë§· (2ê°œì›” ê°„ê²©)
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    plt.tight_layout()
    plt.savefig('backtest_results.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    return sharpe, sortino, mdd

# --- [ìˆ˜ì •] 4ê°œ ì—ì´ì „íŠ¸ì˜ ì‹ í˜¸ ë³€í™˜ ---
def convert_joint_action_to_signal(joint_action, action_map):
    action_to_score = {"Long": 1, "Hold": 0, "Short": -1}
    # (joint_actionì€ (a0, a1, a2, a3) íŠœí”Œì´ ë¨)
    score = sum(action_to_score[action_map[a]] for a in joint_action)
    
    if score >= 3:
        return "ì ê·¹ ë§¤ìˆ˜"
    elif score == 2 or score == 1:
        return "ë§¤ìˆ˜"
    elif score == 0:
        return "ë³´ìœ "
    elif score == -1 or score == -2:
        return "ë§¤ë„"
    elif score <= -3:
        return "ì ê·¹ ë§¤ë„"
    return "ë³´ìœ " # ê¸°ë³¸ê°’

# --- (generate_ai_explanation í•¨ìˆ˜ëŠ” ìˆ˜ì • ë¶ˆí•„ìš”) ---
def generate_ai_explanation(final_signal, agent_analyses):
    all_importances = {}
    for _, _, importance_list in agent_analyses:
        for feature, imp in importance_list:
            all_importances[feature] = all_importances.get(feature, 0.0) + imp
            
    sorted_features = sorted(all_importances.items(), key=lambda item: item[1], reverse=True)
    
    explanation = f"AIê°€ '{final_signal}'ì„ ê²°ì •í•œ ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n"
    
    if not sorted_features:
        return explanation + "ë°ì´í„° ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."
        
    top_feature_1 = sorted_features[0][0]
    explanation += f"  1. '{top_feature_1}' ì§€í‘œì˜ ìµœê·¼ ì›€ì§ì„ì„ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤.\n"
    
    if len(sorted_features) > 1:
        top_feature_2 = sorted_features[1][0]
        explanation += f"  2. '{top_feature_2}' ì§€í‘œê°€ 2ìˆœìœ„ë¡œ ê²°ì •ì— ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤.\n"
        
    if len(sorted_features) > 2:
        top_feature_3 = sorted_features[2][0]
        explanation += f"  3. ë§ˆì§€ë§‰ìœ¼ë¡œ '{top_feature_3}' ì§€í‘œë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.\n"
        
    return explanation

# --- UI ì¶œë ¥ í•¨ìˆ˜ ---
def print_ui_output(
    final_signal, 
    ai_explanation, 
    current_indicators, 
    best_q_total_value
):
    print("\n\n=============================================")
    print("      [ ğŸ“± ë¦¬ë¸Œë¦¬ AI ë¶„ì„ ê²°ê³¼ (ì‚¼ì„±ì „ì) ]")
    print("=============================================")
    
    print("\n--- 1. AI ìµœì¢… ì‹ í˜¸ ---")
    print(f"    {final_signal}")
    print(f"    (ì˜ˆìƒ íŒ€ Q-Value: {best_q_total_value:.4f})")
    
    print("\n--- 2. AI ì„¤ëª… ---")
    print(ai_explanation)
    
    print("\n--- 3. ê¸°ìˆ ì  ë¶„ì„ ìƒì„¸ (ìµœì¢…ì¼ ê¸°ì¤€) ---")
    print("    (AIê°€ ì…ìˆ˜í•˜ì—¬ ë¶„ì„í•œ ì›ë³¸ ë°ì´í„°ì…ë‹ˆë‹¤.)\n")
    technical_indicators = [
        'SMA20', 'MACD', 'MACD_Signal', 'RSI', 'Stoch_K', 'Stoch_D', 
        'ATR', 'Bollinger_B', 'VIX'
    ]
    fundamental_indicators = ['ROA', 'DebtRatio', 'AnalystRating']
    
    for indicator in technical_indicators:
        if indicator in current_indicators:
            print(f"    - {indicator:<13}: {current_indicators[indicator]:.2f}")
            
    print("\n    (í€ë”ë©˜íƒˆ ë° ê¸°íƒ€ ë°ì´í„°)\n")
    for indicator in fundamental_indicators:
         if indicator in current_indicators:
            print(f"    - {indicator:<13}: {current_indicators[indicator]:.2f}")
        
    print("=============================================")


# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def main():
    parser = argparse.ArgumentParser(description="QMIX Stock Trading AI")
    parser.add_argument('--capital', type=float, default=10000000, help="íˆ¬ì ê¸ˆì•¡ (ì›) (ì˜ˆ: 10000000 = 1000ë§Œì›)")
    parser.add_argument('--load-model', type=str, default=None, help="í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: qmix_model.pth)")
    parser.add_argument('--skip-training', action='store_true', help="í•™ìŠµ ê±´ë„ˆë›°ê³  ë°±í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰")
    args = parser.parse_args()
    
    # íˆ¬ì ê¸ˆì•¡ ì €ì¥
    CAPITAL = args.capital
    print(f"\n=== íˆ¬ì ì„¤ì • ===")
    print(f"íˆ¬ì ê¸ˆì•¡: {CAPITAL:,.0f}ì›")
            
    # í¬íŠ¸í´ë¦¬ì˜¤ëŠ” í™˜ê²½ì—ì„œ ìë™ ê´€ë¦¬
    user_portfolio = {
        'capital': CAPITAL,
        'positions': [0] * N_AGENTS,
        'entry_prices': [0.0] * N_AGENTS,
        'shares': 0  # ë³´ìœ  ì£¼ì‹ ìˆ˜
    }

    print(f"ì‚¬ìš© ì¥ì¹˜: {DEVICE}")

    processor = DataProcessor()
    
    # [ìˆ˜ì •] 1. processor.process() ë°˜í™˜ê°’ì´ 7ê°œë¡œ ëŠ˜ì–´ë‚¨
    (features_unnormalized_df, prices_df, feature_names,
     agent_0_cols, agent_1_cols, agent_2_cols, agent_3_cols) = processor.process() # <-- ìˆ˜ì •

    # ë°±í…ŒìŠ¤íŒ… ê¸°ê°„: ë§ˆì§€ë§‰ 1ë…„ (252 ê±°ë˜ì¼)
    # í•™ìŠµ ê¸°ê°„: ë‚˜ë¨¸ì§€ ì•½ 10ë…„
    total_days = len(features_unnormalized_df)
    test_days = 252  # 1ë…„ (ì•½ 252 ê±°ë˜ì¼)
    split_idx = total_days - test_days
    
    if split_idx < WINDOW_SIZE * 2:
        print("ì˜¤ë¥˜: ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        return

    train_features_unnorm = features_unnormalized_df.iloc[:split_idx]
    train_prices = prices_df.iloc[:split_idx]
    test_features_unnorm = features_unnormalized_df.iloc[split_idx:]
    test_prices = prices_df.iloc[split_idx:]
    
    print(f"\n--- ë°ì´í„° ë¶„í•  ì •ë³´ ---")
    print(f"ì „ì²´ ë°ì´í„°: {total_days}ì¼")
    print(f"í•™ìŠµ ë°ì´í„°: {len(train_features_unnorm)}ì¼ ({train_prices.index[0]} ~ {train_prices.index[-1]})")
    print(f"ë°±í…ŒìŠ¤íŒ… ë°ì´í„°: {len(test_features_unnorm)}ì¼ ({test_prices.index[0]} ~ {test_prices.index[-1]})")

    # [ìˆ˜ì •] 2. ì •ê·œí™”
    train_features, test_features = processor.normalize_data(
        train_features_unnorm, 
        test_features_unnorm
    )

    # [ìˆ˜ì •] 3. Env ìƒì„±ìì— í”¼ì²˜ ëª©ë¡ ì „ë‹¬ (agent_3_cols ì¶”ê°€)
    train_env = MARLStockEnv(
        train_features, train_prices, 
        agent_0_cols, agent_1_cols, agent_2_cols, agent_3_cols, # <--- ìˆ˜ì •
        n_agents=N_AGENTS, window_size=WINDOW_SIZE
    )
    test_env = MARLStockEnv(
        test_features, test_prices, 
        agent_0_cols, agent_1_cols, agent_2_cols, agent_3_cols, # <--- ìˆ˜ì •
        n_agents=N_AGENTS, window_size=WINDOW_SIZE
    )
    
    # [ìˆ˜ì •] 4. obs_dimì„ 4ê°œ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬
    obs_dim_0 = train_env.observation_dim_0
    obs_dim_1 = train_env.observation_dim_1
    obs_dim_2 = train_env.observation_dim_2
    obs_dim_3 = train_env.observation_dim_3 # <--- ì¶”ê°€
    obs_dims_list = [obs_dim_0, obs_dim_1, obs_dim_2, obs_dim_3] # <--- ìˆ˜ì •
    
    state_dim = train_env.state_dim
    action_dim = train_env.action_dim
    n_features = train_env.n_features_global

    # [ìˆ˜ì •] 5. Learnerì— obs_dims_list ì „ë‹¬
    learner = QMIX_Learner(obs_dims_list, action_dim, state_dim, DEVICE)
    
    # ëª¨ë¸ ë¡œë“œ ì˜µì…˜ ì²˜ë¦¬
    if args.load_model:
        print(f"\n--- í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì¤‘: {args.load_model} ---")
        learner.load_model(args.load_model)
        if args.skip_training:
            print("--- í•™ìŠµ ê±´ë„ˆë›°ê¸° (ë°±í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰) ---")
        else:
            print("--- ì¶”ê°€ í•™ìŠµ ì§„í–‰ ---")
    
    # í•™ìŠµ ìˆ˜í–‰ (skip_trainingì´ Falseì¼ ë•Œë§Œ)
    if not args.skip_training:
        buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, DEVICE)
        total_steps = 0
        
        print(f"\n--- QMIX {NUM_EPISODES} ì—í”¼ì†Œë“œ í•™ìŠµ ì‹œì‘ (ì´ ì§€í‘œ: {n_features}ê°œ) ---")
        # [ìˆ˜ì •] Obs ì°¨ì› 4ê°œ ì¶œë ¥
        print(f"--- Obs ì°¨ì›: A0={obs_dim_0} (ë‹¨ê¸°), A1={obs_dim_1} (ì¥ê¸°), A2={obs_dim_2} (ìœ„í—˜), A3={obs_dim_3} (ê°ì„±) | ê¸€ë¡œë²Œ ìƒíƒœ ì°¨ì›: {state_dim} ---")
        
        # (í•™ìŠµ ë£¨í”„ëŠ” N_AGENTS=3ìœ¼ë¡œ ì¼ë°˜í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ìˆ˜ì • ë¶ˆí•„ìš”)
        for i_episode in range(NUM_EPISODES):
            obs_dict, info = train_env.reset(initial_portfolio=None) 
            global_state = info["global_state"]
            episode_team_reward = 0.0
            done = False
            
            while not done:
                total_steps += 1
                epsilon = max(0.01, 1.0 - total_steps / 50000)

                
                actions_dict = learner.select_actions(obs_dict, epsilon)
                next_obs_dict, rewards_dict, dones_dict, _, info = train_env.step(actions_dict)
                
                next_global_state = info["global_state"]
                team_reward = rewards_dict['agent_0']
                done = dones_dict['__all__']
                
                buffer.add(global_state, obs_dict, actions_dict, team_reward, 
                           next_global_state, next_obs_dict, done)
                           
                learner.train(buffer)
                
                episode_team_reward += team_reward
                obs_dict = next_obs_dict
                global_state = next_global_state

                if total_steps % TARGET_UPDATE_FREQ == 0:
                    learner.update_target_networks()

            if (i_episode + 1) % 1 == 0:
                print(f"Episode {i_episode+1}/{NUM_EPISODES} | Epsilon: {epsilon:.3f} | Team Reward: {episode_team_reward:.2f}")

        print("--- í•™ìŠµ ì™„ë£Œ ---")
        
        # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
        learner.save_model('qmix_model.pth')
    else:
        print("\n--- í•™ìŠµ ê±´ë„ˆëœ€ (ê¸°ì¡´ ëª¨ë¸ ì‚¬ìš©) ---")

    print("\n--- [1] ì „ì²´ í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë°±í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì¤‘ ---")
    print(f"--- ì´ˆê¸° íˆ¬ì ê¸ˆì•¡: {CAPITAL:,.0f}ì› ---")
        
    obs_dict, info = test_env.reset(initial_portfolio=user_portfolio)
    global_state = info["global_state"]
    all_team_rewards = []
    all_raw_pnls = []  # ì‹¤ì œ ê¸ˆì•¡ ê¸°ì¤€ ìˆ˜ìµ ì¶”ì 
    portfolio_values = [CAPITAL]  # í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ì¶”ì 
    current_step = 0
    while current_step < test_env.max_steps:
        actions_dict = learner.select_actions(obs_dict, 0.0) # Epsilon = 0.0
        obs_dict, rewards_dict, dones_dict, _, info = test_env.step(actions_dict)
        all_team_rewards.append(rewards_dict['agent_0'])
        all_raw_pnls.append(info["raw_pnl"])  # ì‹¤ì œ ê¸ˆì•¡ ìˆ˜ìµ ì €ì¥
        portfolio_values.append(info["portfolio_value"])
        global_state = info["global_state"]
        current_step += 1
        if dones_dict['__all__']:
            break
    
    final_portfolio_value = portfolio_values[-1]
    final_shares = info["shares"]
    final_cash = info["cash"]

    print("\n--- [2] ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì§€í‘œ (ì‹ ë¢°ë„/ì •í™•ë„) ---")
    test_days = len(all_team_rewards)
    if test_days > 0:
        all_rewards_series = pd.Series(all_team_rewards)
        all_raw_pnls_series = pd.Series(all_raw_pnls)  # ì‹¤ì œ ê¸ˆì•¡ ì‹œë¦¬ì¦ˆ
        
        total_pnl = all_raw_pnls_series.sum()  # ì‹¤ì œ ê¸ˆì•¡ ê¸°ì¤€ ëˆ„ì  ìˆ˜ìµ
        daily_avg_pnl = all_raw_pnls_series.mean()  # ì‹¤ì œ ê¸ˆì•¡ ê¸°ì¤€ ì¼ í‰ê· 
        daily_std = all_rewards_series.std() + 1e-9
        sharpe_ratio = (all_rewards_series.mean() / daily_std) * np.sqrt(252)
        win_days = (all_raw_pnls_series > 0).sum()  # ì‹¤ì œ ìˆ˜ìµ ê¸°ì¤€ ìŠ¹ë¥ 
        win_rate = (win_days / test_days) * 100.0
        
        total_return_pct = ((final_portfolio_value - CAPITAL) / CAPITAL) * 100
        
        print(f"    - ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„    : {test_days} ì¼")
        print(f"    - ì´ˆê¸° íˆ¬ì ê¸ˆì•¡   : {CAPITAL:,.0f} ì›")
        print(f"    - ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤  : {final_portfolio_value:,.0f} ì›")
        print(f"    - ë³´ìœ  ì£¼ì‹        : {final_shares} ì£¼")
        print(f"    - ë³´ìœ  í˜„ê¸ˆ        : {final_cash:,.0f} ì›")
        print(f"    - ëˆ„ì  ìˆ˜ìµ(PnL)   : {total_pnl:,.0f} ì› ({total_return_pct:+.2f}%)")
        print(f"    - ì¼ í‰ê·  ìˆ˜ìµ     : {daily_avg_pnl:,.0f} ì›")
        print(f"    - ì¼ ìˆ˜ìµ ë³€ë™ì„±   : {daily_std:.4f}")
        print(f"    - ìƒ¤í”„ ë¹„ìœ¨ (ì—°í™˜ì‚°): {sharpe_ratio:.3f}")
        print(f"    - ìŠ¹ë¥  (ì¼ë³„)      : {win_rate:.2f}% ({win_days}/{test_days}ì¼)")
        
        # ê·¸ë˜í”„ ì¶œë ¥
        print("\n--- [3] ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê·¸ë˜í”„ ìƒì„± ì¤‘ ---")
        graph_sharpe, graph_sortino, graph_mdd = plot_backtest_results(portfolio_values, all_raw_pnls, test_prices, CAPITAL)
        print(f"    Sharpe Ratio: {graph_sharpe:.3f}")
        print(f"    Sortino Ratio: {graph_sortino:.3f}")
        print(f"    MDD: {graph_mdd:.2f}%")
        print("    ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: backtest_results.png")
    else:
        print("    - ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ì´ 0ì¼ì´ì–´ì„œ ì„±ëŠ¥ì„ ì¸¡ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # --- [3] ìµœì¢…ì¼ ìƒì„¸ ë¶„ì„ ---
    print("\n--- [3] ìµœì¢…ì¼ ì˜ˆì¸¡ ìƒì„¸ ë¶„ì„ ---")
    
    final_obs_dict = obs_dict
    action_map = {0: "Long", 1: "Hold", 2: "Short"}
    action_indices = list(action_map.keys())
    action_names = list(action_map.values())
    
    obs_tensors = [torch.FloatTensor(final_obs_dict[f'agent_{i}']).unsqueeze(0).to(DEVICE) for i in range(N_AGENTS)]
    state_tensor = torch.FloatTensor(global_state).unsqueeze(0).to(DEVICE)
    
    q_vals_all_agents = []
    with torch.no_grad():
        for i, agent in enumerate(learner.agents):
            q_vals_all_agents.append(agent.get_q_values(obs_tensors[i]))

    # --- [ìˆ˜ì •] 3D ê·¸ë¦¬ë“œ ê³„ì‚° (3ì¤‘ for-loop) ---
    agent_q_inputs = []
    action_tuples = [] # (a0, a1, a2)
    
    q_vals_0 = q_vals_all_agents[0].squeeze(0)
    q_vals_1 = q_vals_all_agents[1].squeeze(0)
    q_vals_2 = q_vals_all_agents[2].squeeze(0)
    q_vals_3 = q_vals_all_agents[3].squeeze(0) # <-- ì¶”ê°€

    for i, a0_idx in enumerate(action_indices):
        for j, a1_idx in enumerate(action_indices):
            for k, a2_idx in enumerate(action_indices):
                for l, a3_idx in enumerate(action_indices): # <-- ì¶”ê°€
                    q0 = q_vals_0[a0_idx]
                    q1 = q_vals_1[a1_idx]
                    q2 = q_vals_2[a2_idx]
                    q3 = q_vals_3[a3_idx] # <-- ì¶”ê°€
                    agent_q_inputs.append(torch.stack([q0, q1, q2, q3])) # <-- ìˆ˜ì •
                    action_tuples.append((a0_idx, a1_idx, a2_idx, a3_idx)) # <-- ìˆ˜ì •
    
    agent_q_batch = torch.stack(agent_q_inputs) 
    state_batch = state_tensor.repeat(len(action_tuples), 1)

    with torch.no_grad():
        all_q_totals = learner.mixer(agent_q_batch, state_batch)
    
    # [ìˆ˜ì •] ê·¸ë¦¬ë“œë¥¼ 4D (A0, A1, A2, A3)ë¡œ ë³€ê²½
    q_total_grid = all_q_totals.view(
        len(action_indices), len(action_indices), len(action_indices), len(action_indices)
    ).cpu().numpy()
    
    best_q_total_value = all_q_totals.max().item()
    best_joint_action_idx_flat = all_q_totals.argmax().item()
    best_joint_action_indices = action_tuples[best_joint_action_idx_flat] # (a0, a1, a2) íŠœí”Œ
    
    # --- [ìˆ˜ì •] XAI íŒŒíŠ¸ 4ê°œ ì—ì´ì „íŠ¸ ë¦¬ìŠ¤íŠ¸ ---
    agent_analyses = []
    feature_names_list = [agent_0_cols, agent_1_cols, agent_2_cols, agent_3_cols] # <-- ìˆ˜ì •
    n_features_list = [
        train_env.n_features_agent_0, 
        train_env.n_features_agent_1, 
        train_env.n_features_agent_2,
        train_env.n_features_agent_3 # <-- ì¶”ê°€
    ]
    
    for i, agent in enumerate(learner.agents):
        obs = final_obs_dict[f'agent_{i}']
        agent_feature_names = feature_names_list[i]
        n_features_agent = n_features_list[i]

        action_idx, q_values, importance = agent.get_prediction_with_reason(
            obs, 
            agent_feature_names,
            WINDOW_SIZE, 
            n_features_agent
        )
        agent_analyses.append((action_idx, q_values, importance))
        
    final_signal = convert_joint_action_to_signal(best_joint_action_indices, action_map)
    ai_explanation = generate_ai_explanation(final_signal, agent_analyses)
    
    current_indicator_values = test_features_unnorm.iloc[-1]
    
    # --- UI í¬ë§·ìœ¼ë¡œ ì¶œë ¥ ---
    print_ui_output(
        final_signal=final_signal,
        ai_explanation=ai_explanation,
        current_indicators=current_indicator_values,
        best_q_total_value=best_q_total_value
    )

if __name__ == "__main__":
    main()